{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOAVYABGK8SOJgPBNTxlP3S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabhaloneai/ALGO/blob/main/MCST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Below is a high-level implementation of one simulation of the search algorithm.\n",
        "\n",
        "def search(s, game, nnet):\n",
        "    if game.gameEnded(s): return -game.gameReward(s)\n",
        "\n",
        "    if s not in visited:\n",
        "        visited.add(s)\n",
        "        P[s], v = nnet.predict(s)\n",
        "        return -v\n",
        "\n",
        "    max_u, best_a = -float(\"inf\"), -1\n",
        "    for a in game.getValidActions(s):\n",
        "        u = Q[s][a] + c_puct*P[s][a]*sqrt(sum(N[s]))/(1+N[s][a])\n",
        "        if u>max_u:\n",
        "            max_u = u\n",
        "            best_a = a\n",
        "    a = best_a\n",
        "\n",
        "    sp = game.nextState(s, a)\n",
        "    v = search(sp, game, nnet)\n",
        "\n",
        "    Q[s][a] = (N[s][a]*Q[s][a] + v)/(N[s][a]+1)\n",
        "    N[s][a] += 1\n",
        "    return -v"
      ],
      "metadata": {
        "id": "QfiQFvtfQaBK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wcs8rfqaQg-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The high-level code for the complete training algorithm is provided below.\n",
        "\n",
        "def policyIterSP(game):\n",
        "    nnet = initNNet()                                       # initialise random neural network\n",
        "    examples = []\n",
        "    for i in range(numIters):\n",
        "        for e in range(numEps):\n",
        "            examples += executeEpisode(game, nnet)          # collect examples from this game\n",
        "        new_nnet = trainNNet(examples)\n",
        "        frac_win = pit(new_nnet, nnet)                      # compare new net with previous net\n",
        "        if frac_win > threshold:\n",
        "            nnet = new_nnet                                 # replace with new net\n",
        "    return nnet\n",
        "\n",
        "def executeEpisode(game, nnet):\n",
        "    examples = []\n",
        "    s = game.startState()\n",
        "    mcts = MCTS()                                           # initialise search tree\n",
        "\n",
        "    while True:\n",
        "        for _ in range(numMCTSSims):\n",
        "            mcts.search(s, game, nnet)\n",
        "        examples.append([s, mcts.pi(s), None])              # rewards can not be determined yet\n",
        "        a = random.choice(len(mcts.pi(s)), p=mcts.pi(s))    # sample action from improved policy\n",
        "        s = game.nextState(s,a)\n",
        "        if game.gameEnded(s):\n",
        "            examples = assignRewards(examples, game.gameReward(s))\n",
        "            return examples"
      ],
      "metadata": {
        "id": "SkETk4i6QafH"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}